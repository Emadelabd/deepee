{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About deepee is a library for differentially private deep learning in PyTorch. More precisely, deepee implements the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm originally described by Abadi et al. . Despite the name, deepee works with any (first order) optimizer, including Adam, AdaGrad, etc. It wraps a regular PyTorch model and takes care of calculating per-sample gradients, clipping, noising and accumulating gradients with an API which closely mimics the PyTorch API of the original model. Design principles The DP-SGD algorithm requires two key steps, which differ from \"normal\" neural network training. For every minibatch, the algorithm requires to: Obtain the gradients of each individual sample in the batch to calculate their L2-norm and clip it to a pre-set threshold Average the gradients and add Gaussian noise to the average before taking an optimisation step. The first of these two steps is complicated, as deep learning frameworks like PyTorch are not designed to provide per-sample gradients by default. deepee works around this limitation by creating a \"copy\" of the network for every sample in the batch and then doing a parallel forward and backward pass on each sample simultaneously. This technique has two benefits: deepee works with any neural network architecture that can be defined in PyTorch without any user modification The process is very efficient as it happens in parallel and doesn't create significant memory overhead. The copies of the models are kept as references to the original weights and thus it's not required to create \"real\" model copies. The only memory overhead thus results from keeping the per-sample gradients in memory for a short time during each batch. Basic usage The key component of the framework is the PrivacyWrapper class, which wraps a PyTorch module and takes care of the training process behind the scenes. The clipping norm and noise multiplier parameters can be set here. deepee will automatically check if a model contains incompatible layers (such as Batch Normalisation) and throw an error. If such layers exist in the network, the ModelSurgeon can be used to replace them with compatible layers such as Group Normalisation, Instance Normalisation etc.. deepee also offers automatic privacy accounting and will interrupt the training (and optionally save the last model) when the privacy budget is exhausted. This process is abstracted using the PrivacyWatchdog class. An example for the use of deepee showcasing all these concepts can be found here . For paper readers If you would like to reproduce the experiments from our paper, please switch to the paper branch. Instructions for reproduction can be found in the README of the branch. Installation You can install deepee with: pip install deepee deepee does not come with any hard-coded dependencies to maintain high compatibility, so these must be installed separately. deepee is tested with pytorch>1.8 (CPU and GPU) on Ubuntu Linux (min. 20.04) and MacOS >11.3. No GPU support is available for MacOS. The cryptographically secure random number generator (CSPRNG) used by deepee must also be installed separately. The framework will function without the CSPRNG, however we stress that it should not be used in production environments without this feature. To install torchcsprng , follow the instructions on this page. Lastly, SciPy is required, which can be installed with pip install scipy . For Linux, the full installation can therefore look something like this: pip3 install torchcsprng==0.2.0 torch==1.8.0+cu101 torchvision==0.9.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html pip3 install scipy deepee Contributing deepee is licensed under the Apache 2.0. license. Contributions are welcome via PR. To contribute, please install the following additional dependencies: mypy, black, pytest, testfixtures . Packaging is carried out using poetry .","title":"Home"},{"location":"#about","text":"deepee is a library for differentially private deep learning in PyTorch. More precisely, deepee implements the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm originally described by Abadi et al. . Despite the name, deepee works with any (first order) optimizer, including Adam, AdaGrad, etc. It wraps a regular PyTorch model and takes care of calculating per-sample gradients, clipping, noising and accumulating gradients with an API which closely mimics the PyTorch API of the original model.","title":"About"},{"location":"#design-principles","text":"The DP-SGD algorithm requires two key steps, which differ from \"normal\" neural network training. For every minibatch, the algorithm requires to: Obtain the gradients of each individual sample in the batch to calculate their L2-norm and clip it to a pre-set threshold Average the gradients and add Gaussian noise to the average before taking an optimisation step. The first of these two steps is complicated, as deep learning frameworks like PyTorch are not designed to provide per-sample gradients by default. deepee works around this limitation by creating a \"copy\" of the network for every sample in the batch and then doing a parallel forward and backward pass on each sample simultaneously. This technique has two benefits: deepee works with any neural network architecture that can be defined in PyTorch without any user modification The process is very efficient as it happens in parallel and doesn't create significant memory overhead. The copies of the models are kept as references to the original weights and thus it's not required to create \"real\" model copies. The only memory overhead thus results from keeping the per-sample gradients in memory for a short time during each batch.","title":"Design principles"},{"location":"#basic-usage","text":"The key component of the framework is the PrivacyWrapper class, which wraps a PyTorch module and takes care of the training process behind the scenes. The clipping norm and noise multiplier parameters can be set here. deepee will automatically check if a model contains incompatible layers (such as Batch Normalisation) and throw an error. If such layers exist in the network, the ModelSurgeon can be used to replace them with compatible layers such as Group Normalisation, Instance Normalisation etc.. deepee also offers automatic privacy accounting and will interrupt the training (and optionally save the last model) when the privacy budget is exhausted. This process is abstracted using the PrivacyWatchdog class. An example for the use of deepee showcasing all these concepts can be found here .","title":"Basic usage"},{"location":"#for-paper-readers","text":"If you would like to reproduce the experiments from our paper, please switch to the paper branch. Instructions for reproduction can be found in the README of the branch.","title":"For paper readers"},{"location":"#installation","text":"You can install deepee with: pip install deepee deepee does not come with any hard-coded dependencies to maintain high compatibility, so these must be installed separately. deepee is tested with pytorch>1.8 (CPU and GPU) on Ubuntu Linux (min. 20.04) and MacOS >11.3. No GPU support is available for MacOS. The cryptographically secure random number generator (CSPRNG) used by deepee must also be installed separately. The framework will function without the CSPRNG, however we stress that it should not be used in production environments without this feature. To install torchcsprng , follow the instructions on this page. Lastly, SciPy is required, which can be installed with pip install scipy . For Linux, the full installation can therefore look something like this: pip3 install torchcsprng==0.2.0 torch==1.8.0+cu101 torchvision==0.9.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html pip3 install scipy deepee","title":"Installation"},{"location":"#contributing","text":"deepee is licensed under the Apache 2.0. license. Contributions are welcome via PR. To contribute, please install the following additional dependencies: mypy, black, pytest, testfixtures . Packaging is carried out using poetry .","title":"Contributing"},{"location":"examples/","text":"Examples This page shows the basic utilisation of deepee in a step by step MNIST example. Being by importing the relevant libraries: from deepee import (PrivacyWrapper, PrivacyWatchdog, UniformDataLoader, ModelSurgeon, SurgicalProcedures) import torch from torch import nn from torchvision import datasets, transforms class args: batch_size = 200 test_batch_size = 200 log_interval = 1000 num_epochs = 5 device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device = args.device To train with DP guarantees, a special DataLoader is required. deepee provides this DataLoader with sensible presets: train_loader = UniformDataLoader( datasets.MNIST( \"./data\", train=True, download=True, transform=transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ] ), ), batch_size=args.batch_size, ) test_loader = torch.utils.data.DataLoader( datasets.MNIST( \"./data\", train=False, transform=transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ] ), ), batch_size=args.test_batch_size, shuffle=True, ) Next, define the network architecture: class SimpleNet(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.bn1 = nn.BatchNorm1d(256) self.fc2 = nn.Linear(256, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.flatten(x, 1) x = torch.sigmoid(self.fc1(x)) x = self.bn1(x) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return x To train with DP, we now attach the PrivacyWrapper to the model and set up a PrivacyWatchDog to monitor the privacy loss: watchdog = PrivacyWatchdog( train_loader, target_epsilon=1.0, abort=False, target_delta=1e-5, fallback_to_rdp=False, ) model = PrivacyWrapper(SimpleNet(), args.batch_size, 1.0, 1.0, watchdog=watchdog).to( args.device ) optimizer = torch.optim.SGD(model.wrapped_model.parameters(), lr=0.1) The PrivacyWrapper will throw an error now: --------------------------------------------------------------------------- BadModuleError Traceback (most recent call last) <ipython-input-16-a70572be6dfb> in <module>() 7 ) 8 ----> 9 model = PrivacyWrapper(SimpleNet(), args.batch_size, 1.0, 1.0, watchdog=watchdog).to( 10 args.device 11 ) 1 frames /usr/local/lib/python3.7/dist-packages/deepee/snooper.py in snoop(self, model) 38 msg += validator.validate(model) 39 if msg != \"\": ---> 40 raise BadModuleError(msg) 41 42 BadModuleError: BatchNorm Layers must have track_running_stats turned off, otherwise be replaced with InstanceNorm, LayerNorm or GroupNorm. Luckily, this modification can be easily done using the ModelSurgeon : surgeon = ModelSurgeon(SurgicalProcedures.BN_to_GN) model = surgeon.operate(model) We can now proceed with training as usual: # Train for epoch in range(args.num_epochs): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = torch.nn.CrossEntropyLoss()(output, target) loss.backward() model.clip_and_accumulate() model.noise_gradient() optimizer.step() model.prepare_next_batch() if batch_idx % args.log_interval == 0: print( \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format( epoch, batch_idx * len(data), len(train_loader.dataset), 100.0 * batch_idx / len(train_loader), loss.item(), ) ) # Test model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += torch.nn.CrossEntropyLoss(reduction=\"sum\")( output, target ).item() # sum up batch loss pred = output.argmax( dim=1, keepdim=True ) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print( \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format( test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset), ) ) Train Epoch: 0 [0/60000 (0%)] Loss: 2.317095 INFO:root:Privacy spent at 200 steps: 0.27 INFO:root:Privacy spent at 300 steps: 0.34 INFO:root:Privacy spent at 400 steps: 0.39 Test set: Average loss: 1.6950, Accuracy: 5639/10000 (56%) Train Epoch: 1 [0/60000 (0%)] Loss: 1.667842 INFO:root:Privacy spent at 500 steps: 0.44 INFO:root:Privacy spent at 600 steps: 0.49","title":"Examples"},{"location":"examples/#examples","text":"This page shows the basic utilisation of deepee in a step by step MNIST example. Being by importing the relevant libraries: from deepee import (PrivacyWrapper, PrivacyWatchdog, UniformDataLoader, ModelSurgeon, SurgicalProcedures) import torch from torch import nn from torchvision import datasets, transforms class args: batch_size = 200 test_batch_size = 200 log_interval = 1000 num_epochs = 5 device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device = args.device To train with DP guarantees, a special DataLoader is required. deepee provides this DataLoader with sensible presets: train_loader = UniformDataLoader( datasets.MNIST( \"./data\", train=True, download=True, transform=transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ] ), ), batch_size=args.batch_size, ) test_loader = torch.utils.data.DataLoader( datasets.MNIST( \"./data\", train=False, transform=transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ] ), ), batch_size=args.test_batch_size, shuffle=True, ) Next, define the network architecture: class SimpleNet(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 256) self.bn1 = nn.BatchNorm1d(256) self.fc2 = nn.Linear(256, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.flatten(x, 1) x = torch.sigmoid(self.fc1(x)) x = self.bn1(x) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return x To train with DP, we now attach the PrivacyWrapper to the model and set up a PrivacyWatchDog to monitor the privacy loss: watchdog = PrivacyWatchdog( train_loader, target_epsilon=1.0, abort=False, target_delta=1e-5, fallback_to_rdp=False, ) model = PrivacyWrapper(SimpleNet(), args.batch_size, 1.0, 1.0, watchdog=watchdog).to( args.device ) optimizer = torch.optim.SGD(model.wrapped_model.parameters(), lr=0.1) The PrivacyWrapper will throw an error now: --------------------------------------------------------------------------- BadModuleError Traceback (most recent call last) <ipython-input-16-a70572be6dfb> in <module>() 7 ) 8 ----> 9 model = PrivacyWrapper(SimpleNet(), args.batch_size, 1.0, 1.0, watchdog=watchdog).to( 10 args.device 11 ) 1 frames /usr/local/lib/python3.7/dist-packages/deepee/snooper.py in snoop(self, model) 38 msg += validator.validate(model) 39 if msg != \"\": ---> 40 raise BadModuleError(msg) 41 42 BadModuleError: BatchNorm Layers must have track_running_stats turned off, otherwise be replaced with InstanceNorm, LayerNorm or GroupNorm. Luckily, this modification can be easily done using the ModelSurgeon : surgeon = ModelSurgeon(SurgicalProcedures.BN_to_GN) model = surgeon.operate(model) We can now proceed with training as usual: # Train for epoch in range(args.num_epochs): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = torch.nn.CrossEntropyLoss()(output, target) loss.backward() model.clip_and_accumulate() model.noise_gradient() optimizer.step() model.prepare_next_batch() if batch_idx % args.log_interval == 0: print( \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format( epoch, batch_idx * len(data), len(train_loader.dataset), 100.0 * batch_idx / len(train_loader), loss.item(), ) ) # Test model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += torch.nn.CrossEntropyLoss(reduction=\"sum\")( output, target ).item() # sum up batch loss pred = output.argmax( dim=1, keepdim=True ) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print( \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format( test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset), ) ) Train Epoch: 0 [0/60000 (0%)] Loss: 2.317095 INFO:root:Privacy spent at 200 steps: 0.27 INFO:root:Privacy spent at 300 steps: 0.34 INFO:root:Privacy spent at 400 steps: 0.39 Test set: Average loss: 1.6950, Accuracy: 5639/10000 (56%) Train Epoch: 1 [0/60000 (0%)] Loss: 1.667842 INFO:root:Privacy spent at 500 steps: 0.44 INFO:root:Privacy spent at 600 steps: 0.49","title":"Examples"}]}